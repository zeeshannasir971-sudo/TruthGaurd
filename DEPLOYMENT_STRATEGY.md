# TruthGuard Deployment Strategy for Vercel/Netlify

## ğŸ¯ Problem: Large Files Don't Work on Vercel/Netlify

Vercel and Netlify have limitations:
- âŒ No Git LFS support during build
- âŒ Cannot commit large .pkl files (50MB+)
- âŒ Cannot commit large .csv files (100MB+)
- âŒ Serverless functions have size limits

## âœ… Solution: Separate Code from Assets

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GitHub Repository                    â”‚
â”‚  âœ… Frontend code                                        â”‚
â”‚  âœ… Backend/API code                                     â”‚
â”‚  âœ… ML inference logic                                   â”‚
â”‚  âœ… Model loading code                                   â”‚
â”‚  âœ… Asset download script                                â”‚
â”‚  âŒ NO .pkl files                                        â”‚
â”‚  âŒ NO .csv files                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Hugging Face Hub / Google Drive             â”‚
â”‚  ğŸ“¦ fake_news_model.pkl (50MB)                          â”‚
â”‚  ğŸ“¦ tfidf_word_vectorizer.pkl (20MB)                    â”‚
â”‚  ğŸ“¦ tfidf_char_vectorizer.pkl (15MB)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Deployment (Vercel/Netlify)             â”‚
â”‚  1. Clone code from GitHub                               â”‚
â”‚  2. Run download_assets.py                               â”‚
â”‚  3. Download models from Hugging Face                    â”‚
â”‚  4. Load models into memory                              â”‚
â”‚  5. Start serving API                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ What Goes Where

### âœ… Commit to GitHub (Small Files)

```
project/
â”œâ”€â”€ app.py                          # Flask entry point
â”œâ”€â”€ startup.py                      # Model download on startup
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ vercel.json                     # Vercel config
â”œâ”€â”€ netlify.toml                    # Netlify config
â”œâ”€â”€ .gitignore                      # Excludes large files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ pipeline.py            # ML logic (with lazy loading)
â”‚   â”‚   â””â”€â”€ artifacts/
â”‚   â”‚       â””â”€â”€ .gitkeep           # Keep directory structure
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ fetch.py
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â”œâ”€â”€ sentiment.py
â”‚   â”‚   â””â”€â”€ search.py
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ download_assets.py     # Downloads models
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ .gitkeep
â””â”€â”€ frontend/                       # Next.js app
```

### âŒ DO NOT Commit (Large Files)

```
âŒ src/ml/artifacts/*.pkl           # Model files (50MB+)
âŒ src/data/*.csv                   # Dataset files (100MB+)
âŒ *.h5, *.hdf5                     # Other ML formats
```

## ğŸš€ Step-by-Step Deployment Process

### Step 1: Upload Models to Hugging Face Hub

#### Option A: Hugging Face Hub (Recommended)

1. **Create Hugging Face Account**
   - Go to https://huggingface.co/join
   - Create account

2. **Create New Model Repository**
   ```bash
   # Install Hugging Face CLI
   pip install huggingface_hub
   
   # Login
   huggingface-cli login
   
   # Create repository
   huggingface-cli repo create truthguard-models --type model
   ```

3. **Upload Model Files**
   ```bash
   # Upload each model file
   huggingface-cli upload YOUR_USERNAME/truthguard-models \
     src/ml/artifacts/fake_news_model.pkl
   
   huggingface-cli upload YOUR_USERNAME/truthguard-models \
     src/ml/artifacts/tfidf_word_vectorizer.pkl
   
   huggingface-cli upload YOUR_USERNAME/truthguard-models \
     src/ml/artifacts/tfidf_char_vectorizer.pkl
   ```

4. **Get Public URLs**
   ```
   https://huggingface.co/YOUR_USERNAME/truthguard-models/resolve/main/fake_news_model.pkl
   https://huggingface.co/YOUR_USERNAME/truthguard-models/resolve/main/tfidf_word_vectorizer.pkl
   https://huggingface.co/YOUR_USERNAME/truthguard-models/resolve/main/tfidf_char_vectorizer.pkl
   ```

5. **Update download_assets.py**
   ```python
   HUGGINGFACE_BASE_URL = "https://huggingface.co/YOUR_USERNAME/truthguard-models/resolve/main"
   ```

#### Option B: Google Drive (Alternative)

1. **Upload Files to Google Drive**
   - Upload each .pkl file
   - Right-click â†’ Share â†’ Get link
   - Set to "Anyone with the link can view"

2. **Get File IDs**
   ```
   https://drive.google.com/file/d/FILE_ID_HERE/view?usp=sharing
   ```

3. **Update download_assets.py**
   ```python
   MODEL_FILES = {
       "fake_news_model.pkl": {
           "url": f"{GOOGLE_DRIVE_BASE}YOUR_FILE_ID_1",
       },
       # ... other files
   }
   ```

### Step 2: Update Configuration

1. **Edit src/scripts/download_assets.py**
   - Replace `YOUR_USERNAME` with your Hugging Face username
   - Or add Google Drive file IDs

2. **Test Locally**
   ```bash
   # Delete local models
   rm src/ml/artifacts/*.pkl
   
   # Test download
   python src/scripts/download_assets.py
   
   # Should download all models successfully
   ```

### Step 3: Deploy Backend

#### Option A: Vercel (Recommended for Flask)

1. **Install Vercel CLI**
   ```bash
   npm i -g vercel
   ```

2. **Deploy**
   ```bash
   vercel
   ```

3. **Set Environment Variables** (if needed)
   ```bash
   vercel env add HUGGINGFACE_TOKEN
   ```

4. **Get Backend URL**
   ```
   https://your-project.vercel.app
   ```

#### Option B: Railway (Alternative)

1. **Connect GitHub**
   - Go to https://railway.app
   - Connect repository

2. **Configure**
   - Build command: `pip install -r requirements.txt && python src/scripts/download_assets.py`
   - Start command: `gunicorn app:app`

3. **Deploy**
   - Automatic on push

#### Option C: Render (Alternative)

1. **Create Web Service**
   - Go to https://render.com
   - New â†’ Web Service

2. **Configure**
   - Build: `pip install -r requirements.txt && python src/scripts/download_assets.py`
   - Start: `gunicorn app:app`

### Step 4: Deploy Frontend

1. **Update API URL**
   ```bash
   cd frontend
   echo "NEXT_PUBLIC_API_URL=https://your-backend.vercel.app" > .env.production
   ```

2. **Deploy to Vercel**
   ```bash
   vercel --prod
   ```

3. **Or Deploy to Netlify**
   ```bash
   netlify deploy --prod
   ```

## ğŸ”„ How It Works in Production

### First Deployment

```
1. Vercel/Netlify clones your GitHub repo
   âœ… Gets all code
   âŒ No .pkl files (excluded by .gitignore)

2. Runs build command
   âœ… pip install -r requirements.txt
   âœ… python src/scripts/download_assets.py
   
3. download_assets.py runs
   âœ… Creates src/ml/artifacts/ directory
   âœ… Downloads fake_news_model.pkl from Hugging Face
   âœ… Downloads tfidf_word_vectorizer.pkl
   âœ… Downloads tfidf_char_vectorizer.pkl
   âœ… Verifies all files present

4. App starts
   âœ… startup.py checks models exist
   âœ… Models loaded into memory
   âœ… API ready to serve requests
```

### Subsequent Requests

```
1. User makes API request
   â†“
2. load_model() called
   â†“
3. Check cache
   âœ… Models already in memory
   â†“
4. Return cached models
   â†“
5. Make prediction
   â†“
6. Return result
```

## ğŸ“Š File Sizes

```
GitHub Repository:     ~5 MB (code only)
Hugging Face Hub:      ~85 MB (models)
Deployed App:          ~90 MB (code + downloaded models)
```

## âš ï¸ Common Mistakes to Avoid

### âŒ DON'T: Train model during deployment
```python
# This will FAIL on Vercel/Netlify
def deploy():
    train_model()  # âŒ Too slow, too much memory
    load_model()
```

### âœ… DO: Download pre-trained model
```python
# This WORKS
def deploy():
    download_models()  # âœ… Fast, efficient
    load_model()       # âœ… Load from disk
```

### âŒ DON'T: Load 100MB CSV in serverless function
```python
# This will FAIL
df = pd.read_csv("huge_dataset.csv")  # âŒ Too large
```

### âœ… DO: Use pre-trained model only
```python
# This WORKS
model = load_model()  # âœ… Model already trained
prediction = model.predict(text)  # âœ… Fast inference
```

### âŒ DON'T: Commit large files to Git
```bash
git add src/ml/artifacts/*.pkl  # âŒ Will fail push
```

### âœ… DO: Upload to Hugging Face
```bash
huggingface-cli upload ...  # âœ… Proper storage
```

## ğŸ§ª Testing Before Deployment

### Test 1: Model Download
```bash
# Delete models
rm src/ml/artifacts/*.pkl

# Run download script
python src/scripts/download_assets.py

# Should succeed and download all files
```

### Test 2: App Startup
```bash
# Start app
python app.py

# Should:
# 1. Check for models
# 2. Download if missing
# 3. Start successfully
```

### Test 3: API Request
```bash
# Test prediction
curl -X POST http://localhost:5000/analyze \
  -H "Content-Type: application/json" \
  -d '{"mode":"text","text":"Test article content here..."}'

# Should return prediction
```

## ğŸ“ˆ Performance Expectations

### Cold Start (First Request)
```
Model download:  10-30 seconds (one-time)
Model loading:   2-5 seconds
First prediction: 3-8 seconds total
```

### Warm Requests (Subsequent)
```
Model loading:   0 seconds (cached)
Prediction:      <100ms
Total response:  <200ms
```

## ğŸ¯ Deployment Checklist

- [ ] Models uploaded to Hugging Face Hub
- [ ] download_assets.py updated with correct URLs
- [ ] Tested model download locally
- [ ] .gitignore excludes .pkl and .csv files
- [ ] vercel.json or netlify.toml configured
- [ ] Backend deployed and tested
- [ ] Frontend .env.production updated with backend URL
- [ ] Frontend deployed
- [ ] End-to-end test successful

## ğŸš€ Ready to Deploy!

Your project is now structured correctly for Vercel/Netlify deployment:

1. âœ… Code in GitHub (small)
2. âœ… Models in Hugging Face (large)
3. âœ… Automatic download on deploy
4. âœ… Fast inference in production
5. âœ… No Git LFS needed
6. âœ… No large file commits

**Next Steps:**
1. Upload models to Hugging Face
2. Update download_assets.py URLs
3. Test locally
4. Deploy to Vercel/Netlify
5. Celebrate! ğŸ‰
